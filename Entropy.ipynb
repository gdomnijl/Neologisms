{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "dir_prefix = './data'\n",
    "onlyfiles = [f for f in listdir(dir_prefix) if isfile(join(dir_prefix, f))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('manspread', 25886),\n",
       " ('mansplain', 395244),\n",
       " ('Rendezbooze', 528803),\n",
       " ('hundo', 625630),\n",
       " ('pwned', 1350910),\n",
       " ('knosh', 1763720),\n",
       " ('skrill', 1976074),\n",
       " ('fauxpology', 4257827),\n",
       " ('meep', 4310460),\n",
       " ('spime', 7030250),\n",
       " ('nom', 17815207),\n",
       " ('youthquake', 40565211),\n",
       " ('ninja_sex', 81236344),\n",
       " ('Rickroll', 220321421),\n",
       " ('party_foul', 224477624),\n",
       " ('humblebrag', 357459274),\n",
       " ('man_boobs', 372188635),\n",
       " ('jailbait', 395008876),\n",
       " ('fitspiration', 475787493),\n",
       " ('side_boob', 607069936),\n",
       " ('spliff', 1264216017),\n",
       " ('yeet', 1869224360),\n",
       " ('Nontroversy', 4913077730)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neo_scraped = {}\n",
    "for f in onlyfiles:\n",
    "  neo = re.sub(r'_\\d{4}', '',f)\n",
    "  neo = neo.replace('.json', '')\n",
    "  if neo == 'affluenza (1)':\n",
    "    print(f)\n",
    "  if neo not in neo_scraped:\n",
    "    neo_scraped[neo] = os.stat(join(dir_prefix, f)).st_size\n",
    "  else:\n",
    "    neo_scraped[neo] += os.stat(join(dir_prefix, f)).st_size\n",
    "neo_scraped = sorted(neo_scraped.items(), key=lambda kv: kv[1])\n",
    "neo_scraped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "YEAR_START = 2007\n",
    "YEAR_END = 2019\n",
    "tweet = {}\n",
    "def read_yearly_tweets(word_list):\n",
    "    trouble_shoot_empty = []\n",
    "    entropy_df= pd.DataFrame()\n",
    "    for word in word_list:\n",
    "        print(\"reading %s ...\" % word)\n",
    "           \n",
    "          ## if file is named with word + year\n",
    "        for year in range(YEAR_START, YEAR_END+1):\n",
    "            ##This code works, I commented it out because the local files on my machine done have years 2007 to 2019\n",
    "            #data_file_name = \"./data/\" + word + \"_\" + str(year) + \".json\"\n",
    "            #if os.path.isfile(data_file_name):\n",
    "                #if os.stat(data_file_name).st_size == 0:\n",
    "                    #trouble_shoot_empty.append(word + \"_\" + str(year))\n",
    "            #else:\n",
    "                #with open(data_file_name, 'r') as f: \n",
    "                    #js = pd.read_json(f)\n",
    "                    #entropy_df=pd.concat([entropy_df,js])\n",
    "        #continue\n",
    "      \n",
    "      ## if file is named with just the word \n",
    "            data_file_name = \"./data/\" + word+ \".json\"\n",
    "            \n",
    "            if os.path.isfile(data_file_name):\n",
    "                if os.stat(data_file_name).st_size == 0:\n",
    "                    trouble_shoot_empty.append(word + \"_\" + str(year))\n",
    "                else:\n",
    "            #js = pd.read_json(data_file_name)\n",
    "                    whole_df = pd.read_json(data_file_name)\n",
    "                    with open(data_file_name, 'r') as f: \n",
    "                        js1 = pd.read_json(f)\n",
    "                        js1['neo_word']=word\n",
    "                        entropy_df=pd.concat([entropy_df,js1])\n",
    "                        \n",
    "        \n",
    "    return entropy_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading manspread ...\n",
      "reading mansplain ...\n",
      "reading Rendezbooze ...\n",
      "reading hundo ...\n",
      "reading pwned ...\n",
      "reading knosh ...\n",
      "reading skrill ...\n",
      "reading fauxpology ...\n"
     ]
    }
   ],
   "source": [
    "smallest_k = 8\n",
    "word_ls = [x[0] for x in neo_scraped[:smallest_k]] ## read the k files with the smallest memory\n",
    "#word_ls.remove('affluenza (1)')\n",
    "#word_ls += ['party_foul', 'affluenza', 'man_boobs', 'jailbait', 'genopolitics']# 'photobomb']\n",
    "tweet = read_yearly_tweets(word_ls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mask neologism in tweet text ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask_neo_token(whole_tweet_df):\n",
    "    whole_tweet_df['masked_text'] = whole_tweet_df.apply(lambda x: x.text.lower().replace(x.neo_word.lower(), '_NEOLOGISM_'), axis = 1)\n",
    "    return whole_tweet_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entropy_df=mask_neo_token(tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use spacy tokenizer to tokenize words in masked tweet. \n",
    "# Ouput: Spacy Doc\n",
    "entropy_df['tokens'] = entropy_df['masked_text'].apply(lambda x: nlp.tokenizer(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use regex to tokenize words in masked tweet\n",
    "# Output: list of tokens without punctuation and whitespace\n",
    "entropy_df['tokens_sans_punct'] = entropy_df['masked_text'].apply(lambda x: re.sub(r'[^\\w\\s]','',x).split(\" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Count number of words in each tweet without punctuation and whitespace,  uses regex\n",
    "entropy_df['count'] = entropy_df['masked_text'].apply(lambda x: len(re.sub(r'[^\\w\\s]','',x).split(\" \")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entropy_df.head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Total_count=entropy_df['count'].sum()\n",
    "Total_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make tokens into one big list of words from all tweets\n",
    "from itertools import chain\n",
    "words=list(chain(*entropy_df.tokens_sans_punct))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_Sentences=[t.text for t in entropy_df['tokens']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "def get_counts(tokens):\n",
    "    counts=Counter()\n",
    "    for token in tokens:\n",
    "        counts[token]+=1\n",
    "    return counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get count of all unique words from all tweets\n",
    "token_counts=get_counts(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove '_NEOLOGISM_' and '_NEOLOGISM_ing' and white space from dictionary of word counts\n",
    "token_counts.pop('_NEOLOGISM_', None)\n",
    "token_counts.pop('_NEOLOGISM_ing', None)\n",
    "token_counts.pop('', None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sort count dictionary \n",
    "import operator\n",
    "sorted_x = sorted(token_counts.items(), key=operator.itemgetter(1), reverse=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get dictionary of most K common words\n",
    "K=500\n",
    "most_common_500=dict()\n",
    "for idx, (k, v) in enumerate(sorted_x[:K]):\n",
    "    most_common_500[k]= v/Total_count\n",
    "most_common_500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Group tweets by neologism, and concatenate all words used across tweets for each neologism\n",
    "#Ouput: neologism and all words for that neologism\n",
    "neologism_level_df=entropy_df.groupby(['neo_word'])['tokens_sans_punct'].apply(lambda x: list(chain(*x))).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neologism_level_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Change words list for each neologism to include only unique words\n",
    "#Removes repetition in entropy calculation. If a neologism has more words in it's tweets, it could be skewed if we don't obtain unique words\n",
    "neologism_level_df['tokens_sans_punct']=neologism_level_df['tokens_sans_punct'].apply(lambda x: pd.unique(x).tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove \"_NEOLOGISM_\" and\"_NEOLOGISM_ing\" from list of words for each neologism. \n",
    "neologism_level_df['tokens_sans_punct'] = neologism_level_df.tokens_sans_punct.apply(lambda lst: [x for x in lst if x != \"_NEOLOGISM_\"])\n",
    "neologism_level_df['tokens_sans_punct'] = neologism_level_df.tokens_sans_punct.apply(lambda lst: [x for x in lst if x != \"_NEOLOGISM_ing\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neologism_level_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_neologism_level_list(df):\n",
    "    entropy_list=[]\n",
    "    for i,v in enumerate(df.tokens_sans_punct):\n",
    "        entropy=0\n",
    "        for word in v:\n",
    "            if word in most_common_500.keys():\n",
    "                entropy+=(-most_common_500[word])*np.log2(most_common_500[word])\n",
    "            else:\n",
    "                entropy+=0\n",
    "        entropy_list.append(entropy)\n",
    "    return entropy_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neologism_level_df['entropy']=make_neologism_level_list(neologism_level_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neologism_level_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
